{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Data Exploration\n",
    "\n",
    "This notebook explores the raw data collected from various sources using the project's data collection and cleaning pipeline:\n",
    "1. Historical stock price data from Yahoo Finance\n",
    "2. News data from Alpha Vantage\n",
    "3. Technical indicators and sentiment analysis\n",
    "4. Basic statistical analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/willakins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/willakins/Downloads/Git/MLWebsite/willakins.github.io/notebooks\n",
      "Looking for config file at: /Users/willakins/Downloads/Git/MLWebsite/willakins.github.io/config/config.yaml\n",
      "Config file exists: True\n",
      "Analyzing data for symbols: []\n",
      "Date range: 2020-01-01 to 2024-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/willakins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory and setup paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add SSL certificate fix\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Create absolute path to config file\n",
    "project_root = Path.cwd().parent  # Adjust this if needed to point to your project root\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "\n",
    "print(\"Looking for config file at:\", config_path)\n",
    "print(\"Config file exists:\", config_path.exists())\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.data.data_collection import DataCollector\n",
    "from src.data.data_cleaning import load_config\n",
    "\n",
    "# Initialize data collector with explicit config path\n",
    "collector = DataCollector(config_path=str(config_path))\n",
    "\n",
    "# Pass the config_path to load_config\n",
    "config = load_config(config_path=str(config_path))\n",
    "\n",
    "print(f\"Analyzing data for symbols: {collector.symbols}\")\n",
    "print(f\"Date range: {collector.start_date} to {collector.end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:59:29,759 - src.data.data_collection - ERROR - Error loading config file: [Errno 2] No such file or directory: 'config/config.yaml'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config/config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize data collector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m collector \u001b[38;5;241m=\u001b[39m \u001b[43mDataCollector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use DataCollector class, not data_collection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing data for symbols: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollector\u001b[38;5;241m.\u001b[39msymbols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/Git/MLWebsite/willakins.github.io/src/data/data_collection.py:44\u001b[0m, in \u001b[0;36mDataCollector.__init__\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Initialize the data collector with configuration.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        config_path (str): Path to configuration file\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Access data configuration with error handling\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/Git/MLWebsite/willakins.github.io/src/data/data_collection.py:81\u001b[0m, in \u001b[0;36mDataCollector._load_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load configuration from YAML file.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config/config.yaml'"
     ]
    }
   ],
   "source": [
    "# Initialize data collector\n",
    "collector = DataCollector()  # Use DataCollector class, not data_collection\n",
    "config = load_config()\n",
    "\n",
    "print(f\"Analyzing data for symbols: {collector.symbols}\")\n",
    "print(f\"Date range: {collector.start_date} to {collector.end_date}\")\n",
    "print(f\"Technical indicators enabled: {collector.technical_indicators}\")\n",
    "print(f\"Sentiment analysis enabled: {collector.sentiment_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all data\n",
    "all_data = collector.collect_all_data()\n",
    "\n",
    "# Process each symbol's data\n",
    "processed_data = {}\n",
    "for symbol in collector.symbols:\n",
    "    print(f\"\\nProcessing {symbol} data:\")\n",
    "    \n",
    "    # Clean stock data\n",
    "    stock_df = all_data[symbol]['stock_data']\n",
    "    stock_df['Ticker'] = symbol\n",
    "    cleaned_stock = clean_stock_data(stock_df, config)\n",
    "    print(f\"Stock data shape: {cleaned_stock.shape}\")\n",
    "    \n",
    "    # Clean news data if available\n",
    "    news_df = all_data[symbol]['news_data']\n",
    "    if not news_df.empty and collector.sentiment_enabled:\n",
    "        cleaned_news = clean_news_data(news_df, config)\n",
    "        print(f\"News data shape: {cleaned_news.shape}\")\n",
    "    else:\n",
    "        cleaned_news = None\n",
    "        print(\"No news data available\")\n",
    "    \n",
    "    # Combine data\n",
    "    processed_data[symbol] = combine_data(cleaned_stock, cleaned_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_technical_indicators(df, symbol):\n",
    "    print(f\"\\nTechnical Analysis for {symbol}:\")\n",
    "    \n",
    "    # Plot price and technical indicators\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Price and Moving Averages\n",
    "    ax1.plot(df.index, df['Close'], label='Close')\n",
    "    if 'SMA5' in df.columns:\n",
    "        ax1.plot(df.index, df['SMA5'], label='SMA5')\n",
    "    if 'SMA20' in df.columns:\n",
    "        ax1.plot(df.index, df['SMA20'], label='SMA20')\n",
    "    ax1.set_title(f'{symbol} Price and Moving Averages')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # RSI\n",
    "    if 'RSI' in df.columns:\n",
    "        ax2.plot(df.index, df['RSI'], color='purple')\n",
    "        ax2.axhline(y=70, color='r', linestyle='--')\n",
    "        ax2.axhline(y=30, color='g', linestyle='--')\n",
    "        ax2.set_title('RSI')\n",
    "    \n",
    "    # MACD\n",
    "    if 'MACD' in df.columns and 'MACD_Signal' in df.columns:\n",
    "        ax3.plot(df.index, df['MACD'], label='MACD')\n",
    "        ax3.plot(df.index, df['MACD_Signal'], label='Signal')\n",
    "        ax3.set_title('MACD')\n",
    "        ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for symbol, df in processed_data.items():\n",
    "    analyze_technical_indicators(df, symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_data(df, symbol):\n",
    "    if 'avg_sentiment' not in df.columns:\n",
    "        print(f\"No sentiment data available for {symbol}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nSentiment Analysis for {symbol}:\")\n",
    "    \n",
    "    # Plot sentiment metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Average sentiment over time\n",
    "    ax1.plot(df.index, df['avg_sentiment'], label='Average Sentiment')\n",
    "    ax1.fill_between(df.index, \n",
    "                     df['avg_sentiment'] - df['sentiment_std'],\n",
    "                     df['avg_sentiment'] + df['sentiment_std'],\n",
    "                     alpha=0.2)\n",
    "    ax1.set_title(f'{symbol} Sentiment Over Time')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # News count\n",
    "    ax2.bar(df.index, df['news_count'])\n",
    "    ax2.set_title('Daily News Article Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Summary Statistics:\")\n",
    "    print(df[['avg_sentiment', 'sentiment_std', 'news_count']].describe())\n",
    "\n",
    "for symbol, df in processed_data.items():\n",
    "    analyze_sentiment_data(df, symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "for symbol, df in processed_data.items():\n",
    "    output_path = f'../data/preprocessed/{symbol}_combined_preprocessed.csv'\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Saved processed data for {symbol} to {output_path}\")\n",
    "    \n",
    "    # Print final dataset info\n",
    "    print(f\"\\nFinal dataset info for {symbol}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumns:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
