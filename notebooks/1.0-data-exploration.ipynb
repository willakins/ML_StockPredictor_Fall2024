{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Data Exploration\n",
    "\n",
    "This notebook explores the raw data collected from various sources using the project's data collection and cleaning pipeline:\n",
    "1. Historical stock price data from Yahoo Finance\n",
    "2. News data from Alpha Vantage\n",
    "3. Technical indicators and sentiment analysis\n",
    "4. Basic statistical analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/andresquast/Desktop/4641/stockMarketClone1125/ML_StockPredictor_Fall2024/notebooks\n",
      "Looking for config file at: /Users/andresquast/Desktop/4641/stockMarketClone1125/ML_StockPredictor_Fall2024/config/config.yaml\n",
      "Config file exists: True\n",
      "Analyzing data for symbols: ['AAPL', 'GOOG', 'MSFT']\n",
      "Date range: 2020-01-01 to 2024-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory and setup paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add SSL certificate fix\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Create absolute path to config file\n",
    "project_root = Path.cwd().parent  # Adjust this if needed to point to your project root\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "\n",
    "print(\"Looking for config file at:\", config_path)\n",
    "print(\"Config file exists:\", config_path.exists())\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.data.data_collection import DataCollector\n",
    "from src.data.data_cleaning import load_config\n",
    "\n",
    "# Initialize data collector with explicit config path\n",
    "collector = DataCollector(config_path=str(config_path))\n",
    "\n",
    "# Pass the config_path to load_config\n",
    "config = load_config(config_path=str(config_path))\n",
    "\n",
    "print(f\"Analyzing data for symbols: {collector.symbols}\")\n",
    "print(f\"Date range: {collector.start_date} to {collector.end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collector\n",
    "collector = DataCollector()  # Use DataCollector class, not data_collection\n",
    "config = load_config()\n",
    "\n",
    "print(f\"Analyzing data for symbols: {collector.symbols}\")\n",
    "print(f\"Date range: {collector.start_date} to {collector.end_date}\")\n",
    "print(f\"Technical indicators enabled: {collector.technical_indicators}\")\n",
    "print(f\"Sentiment analysis enabled: {collector.sentiment_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all data\n",
    "all_data = collector.collect_all_data()\n",
    "\n",
    "# Process each symbol's data\n",
    "processed_data = {}\n",
    "for symbol in collector.symbols:\n",
    "    print(f\"\\nProcessing {symbol} data:\")\n",
    "    \n",
    "    # Clean stock data\n",
    "    stock_df = all_data[symbol]['stock_data']\n",
    "    stock_df['Ticker'] = symbol\n",
    "    cleaned_stock = clean_stock_data(stock_df, config)\n",
    "    print(f\"Stock data shape: {cleaned_stock.shape}\")\n",
    "    \n",
    "    # Clean news data if available\n",
    "    news_df = all_data[symbol]['news_data']\n",
    "    if not news_df.empty and collector.sentiment_enabled:\n",
    "        cleaned_news = clean_news_data(news_df, config)\n",
    "        print(f\"News data shape: {cleaned_news.shape}\")\n",
    "    else:\n",
    "        cleaned_news = None\n",
    "        print(\"No news data available\")\n",
    "    \n",
    "    # Combine data\n",
    "    processed_data[symbol] = combine_data(cleaned_stock, cleaned_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_technical_indicators(df, symbol):\n",
    "    print(f\"\\nTechnical Analysis for {symbol}:\")\n",
    "    \n",
    "    # Plot price and technical indicators\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Price and Moving Averages\n",
    "    ax1.plot(df.index, df['Close'], label='Close')\n",
    "    if 'SMA5' in df.columns:\n",
    "        ax1.plot(df.index, df['SMA5'], label='SMA5')\n",
    "    if 'SMA20' in df.columns:\n",
    "        ax1.plot(df.index, df['SMA20'], label='SMA20')\n",
    "    ax1.set_title(f'{symbol} Price and Moving Averages')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # RSI\n",
    "    if 'RSI' in df.columns:\n",
    "        ax2.plot(df.index, df['RSI'], color='purple')\n",
    "        ax2.axhline(y=70, color='r', linestyle='--')\n",
    "        ax2.axhline(y=30, color='g', linestyle='--')\n",
    "        ax2.set_title('RSI')\n",
    "    \n",
    "    # MACD\n",
    "    if 'MACD' in df.columns and 'MACD_Signal' in df.columns:\n",
    "        ax3.plot(df.index, df['MACD'], label='MACD')\n",
    "        ax3.plot(df.index, df['MACD_Signal'], label='Signal')\n",
    "        ax3.set_title('MACD')\n",
    "        ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for symbol, df in processed_data.items():\n",
    "    analyze_technical_indicators(df, symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_data(df, symbol):\n",
    "    if 'avg_sentiment' not in df.columns:\n",
    "        print(f\"No sentiment data available for {symbol}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nSentiment Analysis for {symbol}:\")\n",
    "    \n",
    "    # Plot sentiment metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Average sentiment over time\n",
    "    ax1.plot(df.index, df['avg_sentiment'], label='Average Sentiment')\n",
    "    ax1.fill_between(df.index, \n",
    "                     df['avg_sentiment'] - df['sentiment_std'],\n",
    "                     df['avg_sentiment'] + df['sentiment_std'],\n",
    "                     alpha=0.2)\n",
    "    ax1.set_title(f'{symbol} Sentiment Over Time')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # News count\n",
    "    ax2.bar(df.index, df['news_count'])\n",
    "    ax2.set_title('Daily News Article Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Summary Statistics:\")\n",
    "    print(df[['avg_sentiment', 'sentiment_std', 'news_count']].describe())\n",
    "\n",
    "for symbol, df in processed_data.items():\n",
    "    analyze_sentiment_data(df, symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "for symbol, df in processed_data.items():\n",
    "    output_path = f'../data/preprocessed/{symbol}_combined_preprocessed.csv'\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Saved processed data for {symbol} to {output_path}\")\n",
    "    \n",
    "    # Print final dataset info\n",
    "    print(f\"\\nFinal dataset info for {symbol}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumns:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
